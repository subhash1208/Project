# Admins
Ravi Sharma

Santoshkumar Siripuram

# Main - ROSIE

## Overview

This repository contains source code for the ROSIE project in data-cloud. ROSIE is envisioned to be the intelligent assistant offering within data-cloud and predominant goals are two-fold.

- **Data Driven Insights** - Help Executives and Managers make data-driven decisions.
- **Increase DataCloud adoption** - by helping HR practitioners become more productive in their day-to-day work by making intelligent content recommendations


Accordingly, the code in this repo is organized into two predominant components - **metric recommendations** and **insights**.


# Insights Component
---------------------------
## Location(s)

### Intermediate tables generated by the package

- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_hr\_waf
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_hr\_wef
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_pr_pef
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_tm\_tf
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_dim\_job
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_dim\_labor\_acct\_sec
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_dim\_pay\_grp
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_dim\_paycd\_sec
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_dim\_pers
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_dim\_work\_loc
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_fiscal\_clnts
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_janzz\_job\_titl\_map
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_t\_rpt\_to\_hrchy
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_xtrct\_global\_exclusions
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_hr\_waf
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_hr\_wef
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_tm\_tf
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_hr\_waf\_retn
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_hr\_waf\_mngr
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_fiscal\_qtr
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_hr\_layers\_input
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_hr\_wef\_mngr
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_pr\_pef\_mngr
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_prep\_tm\_tf\_mngr
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_hr\_waf\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_hr\_waf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_hr\_wef\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_hr\_wef\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_pr\_pef\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_pr\_pef\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_tm\_tf\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_cube\_tm\_tf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_hr\_waf\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_hr\_waf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_pr\_pef\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_pr\_pef\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_tm\_tf\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_tm\_tf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_intl\_bm\_hr\_waf\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_intl\_bm\_hr\_waf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_intl\_bm\_pr\_pef\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_intl\_bm\_pr\_pef\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_intl\_bm\_tm\_tf\_manager
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_intl\_bm\_tm\_tf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_bm\_hr\_pr\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_ins\_bm\_hr\_waf\_practitioner
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_mngr\_intl\_bm\_insights
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_prac\_insights
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_prac\_intl\_bm\_insights
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_tm\_mngr\_insights
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_tm\_mngr\_intl\_bm\_insights
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_tm\_prac\_insights
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_exp\_tm\_prac\_intl\_bm\_insights
- \_\_HIVE\_MAIN\_DB\_\_.t\_fact\_bm\_insights
- \_\_HIVE\_MAIN\_DB\_\_.t\_fact\_ins
- \_\_HIVE\_MAIN\_DB\_\_.t\_fact\_insights
- \_\_HIVE\_MAIN\_DB\_\_.t\_fact\_insights\_archive


## Exports to warehouse

- \_\_HIVE\_MAIN\_DB\_\_.t\_fact\_insights


## Maintainers

 - Vamshi Krishna Gunnala - Vamshikrishna.Gunnala@ADP.com
 - Srinivas Kummarapu - srinivasa.kummarapu@adp.com
 - Manoj Oleti - manoj.oleti@adp.com  
 - Pradyumna Mohapatra - pradyumna.mohapatra@adp.com

## Command sequences

1. **MetaData Setup**

   Preparing metadata tables required.Needs to be run only once during RPM install. Further executions are needed ONLY if explicitly requested in instructions.

   ```
   Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/metadata_setup.sh   
   ```
   Example

   ```
   execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/metadata_setup.sh
   ```

2. **Data Retrieval from ADPA for Insights(Except Time)**

   This command retrieves flattened dataset (facts & dims) from warehouse. The extractor shell script supports multiple switches. 

	```
	Usage execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/warehouse_data_extract.sh [options]
	-f	<Datawarehouse Connection Details File>
	-i	<Incremental Load true/false>
	-s	<Yearweek start code(value should be 7 digits. Ex-2017001)>
	-e	<Warehouse Environment> FIT/IAT/PROD etc..
	-c	<CLNT_OBJ_ID(s) to retrieve (comma separated clients list/ALL)> 
	-t	<Starting processing from table>
	-n    <Stop processing to table> 
	-d    <Downstream processing = true/false>
	-p    <Parallel Connections Per DB>
	-m    <Partition multiple during retrieval (advanced)>
	```

	Example: 
	
	```
	FOR FULL LOAD:
	$ execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/warehouse_data_extract.sh -f /home/hdpgwuser/connections/ipe.txt -e IPE -i false
	
	FOR INCREMENTAL LOAD:
	/app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/warehouse_data_extract.sh -f /home/hdpgwuser/connections/ipe.txt -e IPE
	```

3. **Time Data Retrieval from ADPA for Insights**

   This command retrieves flattened time dataset (facts & dims) from warehouse.
	
	```
	Usage execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/warehouse_tm_data_extract_bm.sh [options]
	-f	<Datawarehouse Connection Details File>
	-i	<Incremental Load true/false>
	-s	<Yearweek start code(value should be 7 digits. Ex-2017001)>
	-e	<Warehouse Environment> FIT/IAT/PROD etc..
	-c	<CLNT_OBJ_ID(s) to retrieve (comma separated clients list/ALL)> 
	-p    <Parallel Connections Per DB>
	-m    <Partition multiple during retrieval (advanced)>
	```
	
	Example: 
	
	```
	FOR FULL LOAD:
	$ execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/warehouse_tm_data_extract.sh -f /home/hdpgwuser/connections/ipe.txt -e IPE -i false
	
	FOR INCREMENTAL LOAD:
	/app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/warehouse_tm_data_extract.sh -f /home/hdpgwuser/connections/ipe.txt -e IPE
	```

4. **Prepare Cube inputs**

   Next prepare input tables for generating Cubes.

	```
	Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/warehouse_data_prepare.sh [options]
	    -c	<Using spark resource config>
	    -t	<Starting processing from table>
	    -n        <Stop processing to table> 
	    -d	<Downstream processing = true/false>
	    -e	<Warehouse Environment> FIT/IAT/PROD etc..
	    -i	<Incremental Load true/false>
	    -y	<Year code value from which input data should be prepared(value should be 4 digits. Ex-2017)>
	    -e	<Warehouse Environment> FIT/IAT/PROD etc..      
	```
	
	Example
	
	```
	FOR FULL LOAD:
	execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/warehouse_data_prepare.sh -e IPE -i false
	
	FOR INCREMENTAL LOAD:
	execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/warehouse_data_prepare.sh -e IPE [-y 2018]
	```
	
5. **Generating Cubes**

   Now Generate cubes using the cube input and data and cube config xmls. 

	```
	Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/warehouse_cubes.sh [options]
	    -y	<Generating Cube For Year/s>
	    -c	<Using spark resource config>
	    -t	<Starting processing from table>
	    -n        <Stop processing to table>
	    -e	<Warehouse Environment> FIT/IAT/PROD etc..
	    -i	<Incremental Load true/false>
	    -d	<Downstream processing = true/false>
	```
	
	Example
	
	```
	FOR FULL LOAD:
	execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/warehouse_cubes.sh -e IPE -i false
	
	FOR INCREMENTAL LOAD:
	execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/warehouse_cubes.sh -e IPE [-y 2018]
	```

6. **Build Insights**

   Now build insights using the cubes generated above and insight config xmls. 

	```
	Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/build_insights.sh [options]
	     -c	<Using spark resource config>
	     -t	<Starting processing from table>
	     -n        <Stop processing to table>
	     -d	<Downstream processing = true/false>
	     -e	<Warehouse Environment> FIT/IAT/PROD etc..
	```
	
	Example
	
	```
	execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/build_insights.sh -e IPE
	```

7. **Build Benchmark Insights**

   Now build benchmark insights using the cubes generated above and benchmark insight config xmls. 

	```
	Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/build_insights_bm.sh [options]
	     -c	<Using spark resource config>
	     -t	<Starting processing from table>
	     -d	<Downstream processing = true/false>
	     -e	<Warehouse Environment> FIT/IAT/PROD etc..
	```
	
	Example
	
	```
	execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/build_insights_bm.sh -e IPE
	```

8. **Build Internal Benchmark Insights**

   Now build internal benchmark insights using the cubes generated above and internal benchmark insight config xmls. 

   ```
   Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/build_insights_int_bm.sh [options]
         -c	<Using spark resource config>
         -t	<Starting processing from table>
         -d	<Downstream processing = true/false>
         -e	<Warehouse Environment> FIT/IAT/PROD etc..
   ```

   Example

   ```
   execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/build_insights_int_bm.sh -e IPE
   ```

9. **Prepare export to warehouse**

   Prepare a final insight table to export to data warehouse by combining all intermediate insight tables.

   ```
   Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/prepare_expo.sh [options]
         -e	<Partition Value>
         
   ```

   Example

   ```
   execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/prepare_exports.sh -e IPE
   ```

10. **Export Insights to Warehouse**

   Exporting final insight table to the warehouse.

   ```
   Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/insights/export_insights.sh [options]
   	-f  <Datawarehouse Connection Details File>
   	-p  <Partition value of the table to be exported>   	
   ```
   Example

   ```
   execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/insights/export_insights.sh -f /home/hdpgwuser/connections/ipe.txt -p IPE
   ```




# Metric Recommendations Component
---------------------------
## Location(s)

## Intermediate tables generated by the package

- \_\_HIVE\_MAIN\_DB\_\_.rosie\_metric\_executions
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_report\_executions
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_employee\_jobs
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_metric\_recommendations
- \_\_HIVE\_MAIN\_DB\_\_.rosie\_report\_recommendations

## Exports to warehouse

- \_\_HIVE\_MAIN\_DB\_\_.t\_bnchmrk\_rpt\_rcmndns 
- \_\_HIVE\_MAIN\_DB\_\_.t\_bnchmrk\_mtrc\_rcmndns
- \_\_HIVE\_MAIN\_DB\_\_.t\_rpt\_rcmndns
- \_\_HIVE\_MAIN\_DB\_\_.t\_mtrc\_rcmndns

## Source(s)

- Analytics Metamodel Database(s)
- Reporting Catalog Database(s)
- \_\_HIVE\_MAIN\_DB\_\_.t\_dim\_client\_industries (Generated by ToP processes)
- \_\_HIVE\_BASE\_DB\_\_.employee\_base\_monthly
- \_\_HIVE\_BASE\_DB\_\_.janzz\_result\_monthly
- \_\_HIVE\_BASE\_DB\_\_.client\_core\_master

## Maintainers

 - Manoj Oleti - manoj.oleti@adp.com  
 - Srinivas Kummarapu - srinivasa.kummarapu@adp.com
 - Pradyumna Mohapatra - pradyumna.mohapatra@adp.com

## Command sequences

1. **Data Retrieval from ADPA**

  This command retrieves the list of saved metrics from Analytics Metamodel Database. The extractor shell script supprots multiple switches. If retrieval needs to be made from multiple metamodel databases within the same environment, use "-t false" for all retrieval except the first.

  ```
  Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/recommendations/adpa_data_extract.sh [options]
  	-d	<db_host>
  	-s	<db_service_name>
  	-u	<db_user_name>
  	-u	<db_password>
  	-e	<db_environment> FIT/IAT/PROD etc..
  	-t	<truncate_target_partition>
  ```

  Example: 

  ```
  $ execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/adpa_data_extract.sh -d cdldftef1-scan.es.ad.adp.com -s cri03q_svc1 -u ADPI_APP_USR -p adpiv2 -e FIT
  ```

2. **Data Retrieval from ADPR**

  This command retrieves the list of report executions from Analytics Catalog DBs. The extractor shell script supprots multiple switches. If retrieval needs to be made from multiple catalogs within the same "environment & product", use "-t false" for all retrieval except the first.

  ```

  Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/recommendations/adpr_data_extract.sh [options]
  	-d	<db_host>
  	-s	<db_service_name>
  	-u	<db_user_name>
  	-u	<db_password>
  	-e	<db_environment> FIT/IAT/PROD etc..
  	-t	<truncate_target_partition>
  	-m	<source system> One of vtg/ev5/wfn

  ```

  Example: 

  ```

  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/adpr_data_extract.sh -d cdldftef2-scan.es.ad.adp.com -s crw11q_svc1 -t true -u adprnasr12fit02 -p adprnasr12fit02 -e FIT -m vtg
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/adpr_data_extract.sh -d cdldftef2-scan.es.ad.adp.com -s crw11q_svc1 -t true -u ADPRNASR12FIT03 -p ADPRNASR12FIT03 -e FIT -m ev5
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/adpr_data_extract.sh -d cdldftef2-scan.es.ad.adp.com -s crw11q_svc1 -t true -u adprwfn41fit -p adprwfn41fit -e FIT -m wfn
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/adpr_data_extract.sh -d cdldftef2-scan.es.ad.adp.com -s crw11q_svc1 -t false -u ADPRMASFIT03 -p ADPRMASFIT03 -e FIT -m wfn
  ```

3. **Prepare Employee Jobs dataset**

  Next, retrieve this employee jobs from warehouse if this running from a non-production environment. For PROD, prepare the jobs from the data existing in cluster itself. This dataset is necessary during recommendation generation phase.

  ```

  Usage: execute bash /app/dsenv-<env>/dsmain-rosie/cook/shell/recommendations/export_recommendations.sh [options]
        -c	<db_connect_string>
        -u	<db_user_name>
        -u	<db_password>
        -e	<db_environment> FIT/IAT/PROD etc..
        -t	<target_partition_to_be_exported>
        -s	<source system> One of vtg/ev5/wfn
        
  ```

  Example

  ```
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/prepare_employee_jobs.sh -e PROD

  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/prepare_employee_jobs.sh -d cdldftef1-scan.es.ad.adp.com -s cri03q_svc1 -u ADPIDMCORE01 -p rcsqF7PE6587JsNx -e FITVTG -t true
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/prepare_employee_jobs.sh -d cdldftef1-scan.es.ad.adp.com -s cri03q_svc1 -u ADPIDMCORE02 -p adpivcore02q -e FITWFN -t true
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/prepare_employee_jobs.sh -d cdldftef1-scan.es.ad.adp.com -s cri03q_svc1 -u ADPIDMCORE03 -p adpivcore03q -e FITEV5 -t true
  ```

4. **Generate recommendations**

  Generate recommendations. The "-t" switch is used to provide a partition name in which recommendations need to be prepared. For non-prod environments, simply use the environment name as the partition name (overwriting the partition everytime). For PROD though, use a convention based on the month being run (e.g 2017M09 etc..)

  ```
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/prepare_recommendations.sh -t FIT
  execute bash /app/dsenv-dc1prod/dsmain-rosie/cook/shell/recommendations/prepare_recommendations.sh -t 2017M11
  ```

5. **Data export to warehouse**

  Export recommendations to benchmark schemas & core schemas as appropriate. In the case of core schemas, exports need to be repeated for every core schema in use by the application. (This will be enhanced in ROSIE 2.0 to use parallel exports to multiple schemas simultaneously)

  ```
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/export_recommendations.sh -t FIT -c jdbc:oracle:thin:@cdldftef1-scan.es.ad.adp.com:1521/cri11s_svc1 -u ADPI_BENCHMARK_MODEL -p fitbm490 -s None

  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/export_recommendations.sh -t FIT -c jdbc:oracle:thin:@cdldftef1-scan.es.ad.adp.com:1521/cri11s_svc1 -u ADPIDMCORE01 -p rcsqF7PE6587JsNx -s VTG
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/export_recommendations.sh -t FIT -c jdbc:oracle:thin:@cdldftef1-scan.es.ad.adp.com:1521/cri11s_svc1 -u ADPIDMCORE02 -p adpivcore02q -s WFN
  execute bash /app/dsenv-fitqa1/dsmain-rosie/cook/shell/recommendations/export_recommendations.sh -t FIT -c jdbc:oracle:thin:@cdldftef1-scan.es.ad.adp.com:1521/cri11s_svc1 -u ADPIDMCORE03 -p adpivcore03q -s EV5
  ```

