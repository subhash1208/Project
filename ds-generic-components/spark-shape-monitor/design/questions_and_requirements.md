* Will we be leveraging aws-deequ for metrics collection and validations?
* Whatever is developed, there should be another layer to give useful reporting on validations performed and general shape. Perhaps this is where quicksight will come in?
* The metrics table defined, "TableStatistic", I don't believe is the best way to design the metrics table or at least doesn't meet all requirements. I think we'll be hampering ourselves if we define a static schema like this from the beginning. There could be other metrics that should be captured for a column. For example, Ventures uses the mean(crc32(postal_codes)) or if we want a better percentile granularity. AWS-deequ uses JSON and I think a NOSQL structure may be advantageous here. One metric per row would give us the flexibility we need.
* One thing I do like about ProcessValidator is the RDBMS table structure. One table for metrics, one for validations performed, and one to link the metrics and validations tables to show what metrics were used in a particular validation. The schemas could be simplified but I think it's something that should be borrowed upon.
* Will this be a config driven tool similar to Cube generation and ProcessValidator? I'm not sure there is a viable alternative. That means the tool would be called similar to cube generation. 