# Main - Spark Feature Engineering

## Overview
-------------------

This is a general purpose reusable framework which can be used to generate features using a scalable and reusable specification. 
  
  

## Maintainers
-------------------
 - Manoj Oleti - manoj.oleti@adp.com  
  
  

## How to use
-------------------

This spark-feature-engineering gradle dependency is typically included in your feature engineering project. 

	Feature Engineering v2.0
	Usage: Start [options]
	
	  -n, --application-name <value>
	                           Name of the application.
	  -i, --input-table <value>
	                           Input Table. Fully qualified with domain name.
	  -l, --local-parquet-mode <value>
	                           Treat the inputs and outputs as local/hdfs paths to parquet files. Defaults to false. If true, the input-partitions-* parameters are ignored
	  -o, --output-table <value>
	                           Output Table. Fully qualified with domain name.
	  -g, --grouping-columns <value>
	                           Base columns to be used for group conditions
	  -p, --input-partition-column <value>
	                           Optional partition column to filter the input table with
	  -v, --input-partition-column-values <value>
	                           List of input partition values to query against
	  -c, --output-partition-columns <value>
	                           Partition columns for the generated output table
	  -s, --num-shuffle-partitions <value>
	                           Number of spark shuffle partitions to be used. Defaults to 200
	  -t, --enable-tungsten <value>
	                           Enable/Disable tungsten (true/false). Defaults to true. Disable this Spark 1.5 is being used
	  -r, --reliable-checkpoint-dir <value>
	                           RDD Checkpoint directory. Defaults to /tmp/checkpoints/
	  -f, --force-input-repartition <value>
	                           Force Input data to be repartitioned to match the shuffle partitions
	  -s, --save-format <value>
	                           Output Table format. Supported values are 'parquet' and 'text' 
	  --help                   prints this usage text

Invocation is typically done from within a shell-script. For example..

	safe_call spark-submit --master yarn-cluster --num-executors 70 --executor-cores 3 \
	   	  --executor-memory 16G --driver-memory 32G --driver-cores 4 \
	   	  --conf spark.storage.memoryFraction=0.5 \
	   	  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
	   	  --conf spark.shuffle.blockTransferService=nio \
	   	  --conf spark.rdd.compress=true \
	   	  --conf spark.akka.frameSize=50 \
	   	  --conf spark.network.timeout=480 \
	   	  --conf spark.yarn.executor.memoryOverhead=2400 \
	   	  --conf spark.yarn.driver.memoryOverhead=4800 \
	   	  --conf "spark.executor.extraJavaOptions=-verbose:gc\ -XX:+PrintGCDetails\ -XX:+PrintGCTimeStamps\ -XX:-UseConcMarkSweepGC" \
	   	  --class com.adp.datacloud.tp.Start ../../scala/tp-data-mgmt.jar \
	   	  --application-name "Turnover Probability WFN Feature Engineering" \
	   	  --enable-tungsten true \
	   	  --num-shuffle-partitions 1600 \
	   	  --force-input-repartition true \
	   	  --input-table __HIVE_TP_DB__.all_tp_wfn_data_extract \
	   	  --output-table __HIVE_TP_DB__.tp_wfn_processed_dataset \ 
	   	  --input-partition-column l2_code \
	   	  --input-partition-columns-values 11,21,22 \ 
	   	  --grouping-columns source_system,l2_code,clnt_obj_id \ 
	   	  --output-partition-columns source_system,l2_code

    

## Developer Guide
-------------------

Add the below gradle dependency to your feature engineering project. Ensure that in your gradle script, the class files from spark-feature-engineering dependency are directly bundled into your project jar file. 

	dependencies {
		...
		compile("com.adp.datacloud.ds:spark-feature-engineering:3.0")
		...
	}
	
	jar {
	    baseName = 'tp-data-mgmt'
	    
	    // Add the spark feature engineering jar also to the assembly
		from {
			configurations.compile.filter( { it.name =~ /spark-feature-engineering.*\.jar/ }).collect {
					zipTree(it);
			}
		}
	}

Parse the command-line options of your feature-engineering startup script using "featureEngineeringOptions" object, provided by the above dependency.

	def main(args: Array[String]) {
	  val myConfig = com.adp.datacloud.cli.featureEngineeringOptions.parse(args)
	  ...
	}

Features are generated by simply defining a **FeatureBuilder**. Each feature can be applied one after the other in a chain using this builder.

	val featureBuilder = new FeatureBuilder(dataFrame)
	
	val finalDataset = featureBuilder.apply(feature1).
	   apply(feature2).
	   .... 
	   apply(featureN).toDF()

Multiple types of feature engineering tasks can be done using this module

1. **Categorical Feature Transformations:** Convert free text columns into fixed cardinality categorical variables using simple REGEX transformations. Example...

		<feature name="flsa_stus_dsc" default="EXEMPT">
			<levels>
				<level name="NONEXEMPT">
					<match>no[tn][\s-_]*exempt</match>
					<match>^flsa[\s-_]*required</match>
				</level>
			</levels>
		</feature>

2. **Non-Group Features:** : Generate a new feature by using one ore more existing features. Example a simple ratio of tenure to experience
	
		val feature1 = new NonGroupFeature("tenure_to_experience", safeDivision(1)('tenure, 'experience))
	
3. **Group Features:** : Generate a feature dependent on a group of records instead of a single record. Eg, team count = Number of employees having the same manager.

		val groupingColumns = List("clnt_obj_id")
		
		private val teamCountFeature = new GroupFeature("team_count", 
				groupingColumns ++ List("mngr_pers_obj_id"),
			count("pers_obj_id"))
		
4. **Window Features:** : Features which depend on windows. Eg: is\_at\_risk\_within\_4\_quarters. 

		private val dvTermdInXquarters = new WindowFeature("is_at_risk_within_4_quarters",
			groupingColumns ++ List("pers_obj_id"),
				List(col("qtr_seq_nbr").desc),
					lag(col("is_terminated"), 4,0)
		) }

5. **Missing Value Imputation:** : Imputes missing values for numeric columns using the median. TODO: To be extended for categorical features (using mode)

		val genericMissingValueImputer = new GenericMissingValueImputer(sparkSession, groupingColumns)
		
		val result = (new WorkflowBuilder(sparkSession, dataFrame)).apply(
        	outlierFilter).
        	apply(categoryHandler).
        	apply(featureGenerator).
        	apply(featureFilter).
        	apply(genericMissingValueImputer).
        	toDF


