package com.adp.datacloud.cli

import scala.io.Source

import org.apache.spark.SparkFiles
import com.adp.datacloud.ds.aws.SecretsStore

case class ElasticSearchIndexerConfig(
    file: String                         = "",
    inputParams: List[String]            = List(),
    indexName: String                    = "",
    indexPartitionColumn: Option[String] = None,
    dataRetentionYears: Int              = 2,
    parallelism: Int                     = 5,
    username: Option[String]             = None,
    pwd: Option[String]                  = None,
    mappingId: String                    = "id",
    indexRefreshInterval: Int            = 10, // Refreshes the interval once every 10 seconds.
    disablePipeline: Boolean =
      false, // Disables the "datacloud_pipeline" which is applied by default for all documents
    useForEnrichment: Boolean = false,
    autogenerateId: Boolean   = false,
    files: Option[String]     = None) {
  def sql = {
    val sourcequery = (if (new java.io.File(file).exists()) {
                         // file resides in driver/executor working directory in YARN Cluster mode and hence can be accessed directly
                         Source.fromFile(file)("UTF-8")
                       } else {
                         Source.fromFile(SparkFiles.get(file))("UTF-8")
                       })
      .getLines()
      .map { _.replaceAll("^\\s*\\-\\-.*$", "").replaceAll("^\\s*$", "") }
      .filter(!_.isEmpty())
      .mkString("\n")

    //Replacing the hiveconf variables
    inputParams
      .foldLeft(sourcequery) { (y, x) =>
        ("\\$\\{hiveconf:" + x.split("=")(0) + "\\}").r.replaceAllIn(y, x.split("=")(1))
      }
      .replaceAll("\\$\\{hiveconf:[a-zA-Z0-9]+\\}", "")
  }

  def getParam(paramName: String) = {
    val parameterMap = inputParams.map({ x => x.split("=")(0) -> x.split("=")(1) }).toMap
    parameterMap.get(paramName)
  }

}

object elasticSearchIndexerOptions {

  def parse(args: Array[String]) = {
    val parser =
      new scopt.OptionParser[ElasticSearchIndexerConfig]("ElasticSearch Data Ingestor") {

        head(
          "ElasticSearch Data Ingestor",
          this.getClass.getPackage().getImplementationVersion())
        help("help").text("Prints usage")

        override def showUsageOnError = true

        opt[String]('f', "ingest-sql")
          .required()
          .action((x, c) => c.copy(file = x))
          .text("Input Sql string")

        opt[String]('c', "hiveconf")
          .unbounded()
          .action((x, c) => c.copy(inputParams = c.inputParams ++ List(x)))
          .text("Hiveconf variables to replaced in query")

        opt[String]('i', "index-name")
          .required()
          .action((x, c) => c.copy(indexName = x))
          .text("ElasticSearch Index to write to")

        opt[String]('m', "es-mapping-id")
          .action((x, c) => c.copy(mappingId = x))
          .text("Column to be used as es.mapping.id")

        opt[Int]("indexing-parallelism")
          .action((x, c) => c.copy(parallelism = x))
          .text("Default parallelism for indexing")

        opt[String]("es-username")
          .action((x, c) => c.copy(username = Some(x)))
          .text("Elasticsearch username")

        opt[String]("es-password")
          .action((x, c) => c.copy(pwd = Some(x)))
          .text("Elasticsearch password. Not secure and hence not recommended. By default, password shall be lookedup from Secretstore")

        opt[Boolean]('e', "use-for-enrichment")
          .action((x, c) => c.copy(useForEnrichment = x))
          .text("Use this index for enrichment based on the ID field")

        opt[Boolean]('e', "no-pipeline")
          .action((x, c) => c.copy(disablePipeline = x))
          .text("Disable the use of datacloud_pipeline for ingestion")

        opt[Int]('r', "index-refresh-interval")
          .action((x, c) => c.copy(indexRefreshInterval = x))
          .text("Index Refresh Interval in seconds. Default 10 seconds")

        opt[Int]('r', "data-retention-years")
          .action((x, c) => c.copy(dataRetentionYears = x))
          .text("Index Data Retention in Years (for partitioned indexes).")

        opt[String]('p', "partition-column")
          .action((x, c) => c.copy(indexPartitionColumn = Some(x)))
          .text("Creates a new index for every new partition in this column. Eg: yyyymm")

        opt[Boolean]('a', "auto-generate-id")
          .action((x, c) => c.copy(autogenerateId = x))
          .text(
            "Enable ES's autogenerated ID if the dataframe doesnt have a dedicated 'id' column. Defaults to false")

        opt[String]("files")
          .action((x, c) => c.copy(files = Some(x)))
          .text("csv string dependent files with absolute paths")

      }

    // parser.parse returns Option[C]
    parser.parse(args, ElasticSearchIndexerConfig()) match {
      case Some(config) => config
      case None => {
        System.exit(1)
        null // Error message would have been displayed
      }
    }

  }

}
